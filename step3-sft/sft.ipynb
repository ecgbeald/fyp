{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a18f1c-3158-41a8-98a2-24e61b5faf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, hamming_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566f2cd-9e69-44f4-8d18-02ba6573ca8e",
   "metadata": {},
   "source": [
    "treat the dataset, from raw log into conversational format (only need to run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c5f1af-2ce8-4a72-823a-95b14bb9caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = glob.glob(\"../data/*.csv\")\n",
    "taxonomy_map = {\n",
    "    \"info\": 1,\n",
    "    \"injection\": 2,\n",
    "    \"traversal\": 3,\n",
    "    \"rce\": 4,\n",
    "    \"proxy\": 5,\n",
    "    \"xss\": 6,\n",
    "    \"llm\": 7,\n",
    "    \"other\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754fe789-42cb-47a1-9111-836e3ab0a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(dataset_path):\n",
    "    df = pd.read_csv(dataset_path, skiprows=lambda x: x in range(1), names=['log', 'label', 'category', 'misc'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c73ba5c-af21-4440-a1c2-2e7c18b417f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((load_csv(f) for f in dataset_path), ignore_index=True)\n",
    "df = df.drop(df.columns[3], axis=1)\n",
    "df[\"category\"] = df[\"category\"].apply(    \n",
    "    lambda x: sorted([\n",
    "        taxonomy_map[k.strip().lower()]\n",
    "        for k in str(x).split(',')\n",
    "        if k.strip().lower() in taxonomy_map\n",
    "    ]) if pd.notna(x) else [0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b63606-04e2-46e1-8eea-783b607baa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(log):\n",
    "    user_prompt = \"Given a log entry collected from an Apache HTTP server, classify it as either \\\"Malicious\\\" or \\\"Benign\\\".\\n\\n\\\n",
    "        If the log is classified as malicious, specify the reason(s) (can be multiple) by selecting from the following categories: \\n\\n\\\n",
    "        1. information exposure (reconaissance, scanning)\\n \\\n",
    "        2. injection (including command injection, sql injection, XML external entity attack, shellcode injection)\\n \\\n",
    "        3. path traversal\\n\\\n",
    "        4. remote code execution\\n\\\n",
    "        5. proxy-based attack (Server-Side Request Forgery, open redirect)\\n \\\n",
    "        6. cross site scripting\\n\\\n",
    "        7. prompt injection targeting LLM models\\n\\\n",
    "        8. other (not mentioned above, e.g., local file inclusion, remote file inclusion, etc.)\\n\\n\\\n",
    "        Return your answer in strict JSON format for structured parsing. Use the following format:\\n\\n{{\\n  \\\"classification\\\": \\\"Malicious or Benign\\\",\\n  \\\"reason\\\": \\\"Comma-separated list of category numbers if malicious; leave empty if benign\\\"\\n}}\\n #### Explaination: why the weblog provided is malicious, leave empty if benign.\\n\\\n",
    "        After the JSON briefly explain the reasoning for malicious classifications, if the log is benign, no explanation is needed.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing Apache log entries to detect potential security threats.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt + \"\\n\\\n",
    "        Log:\" + log},\n",
    "    ]\n",
    "    return messages\n",
    "def generate_response(label, category):\n",
    "    if label == 0:\n",
    "        return {\"role\": \"assistant\", \"content\": \"```json {{ \\n \\\"classification\\\" : \\\"Benign\\\", \\n \\\"reason\\\":\\\"\\\"\\n}}\\n```\"}\n",
    "    else:\n",
    "        return {\"role\": \"assistant\", \"content\": f\"```json {{ \\n \\\"classification\\\" : \\\"Malicious\\\", \\n \\\"reason\\\":\\\"{str(category)}\\\"\\n}}\\n```\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b06e157-5763-4b4c-9a84-d919cbd67c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = []\n",
    "for _, row in df.iterrows():\n",
    "    conversation = generate_prompt(row.iloc[0])\n",
    "    conversation.append(generate_response(row.iloc[1], row.iloc[2]))\n",
    "    entry = {\"messages\": conversation}\n",
    "    dicts.append(entry)\n",
    "\n",
    "with open(\"../data/prompt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dicts, f, indent=2, ensure_ascii=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dfd6d-0ff7-4020-b40d-052c3634edda",
   "metadata": {},
   "source": [
    "load dataset from json into huggingface Dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af5e0aa-7a1c-45c6-aedd-fe728020828a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ea337efd874f359e1972a5efbf85d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 2737\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 685\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../data/prompt.json\", split='train')\n",
    "dataset= dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9563bb4-4bbd-4df5-93a0-d457cae0763e",
   "metadata": {},
   "source": [
    "Training step: using LoRA with SFT trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3df91e-f3ed-4c08-bdfa-9a9dcbb54665",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_token\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b8d69f-faff-42fc-89ae-fe6f02a6f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 47725936640}\n"
     ]
    }
   ],
   "source": [
    "max_memory = {0: torch.cuda.get_device_properties(0).total_memory}\n",
    "print(max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46fb5cd-0ef7-40c0-8625-42b97aa84321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97030baf65884f3a9389b1575dfa4e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d515ad5dc0045138fd65a035587e928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/2737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263e4d5258bc40019c30b8182082abc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/2737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903d374539d644fd853224e94209d5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ef844cbcfb49359ddcc0061cd3447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e812ba9da54919b47dfdbbde7e147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/685 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7d32cd2c2e4dfb8ee12876d511a720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/685 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454d7ed059ce490a8dc221fc69ce24da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/685 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed86c8d351834f53b5a6d2c835318a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/685 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "checkpoint='/rds/general/user/rm521/home/fyp/qwen2.5-7B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16, max_memory=max_memory)\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"Qwen2.5-7B-SFT\", \n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=2,\n",
    "    ),\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbfa794-a3d0-4e45-b37b-6ac874deb879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4107' max='4107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4107/4107 45:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4107, training_loss=0.10833801328047424, metrics={'train_runtime': 2741.6288, 'train_samples_per_second': 2.995, 'train_steps_per_second': 1.498, 'total_flos': 1.6382278783785062e+17, 'train_loss': 0.10833801328047424})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2a1c6-7cb1-447f-959a-22619ebfcdef",
   "metadata": {},
   "source": [
    "Evaluation step\n",
    "if model is not loaded in VRAM, reload from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8a6081-456e-4f4c-b56a-04249305f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98bfdaca-3a16-4ba8-b6ed-53ab5fafb44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af551b6eb7843a6aeacfc0548bc7259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = '/rds/general/user/rm521/home/fyp/step3-sft/Qwen2.5-7B-SFT/checkpoint-4107'\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, max_memory=max_memory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eac5b6b-eb87-4530-b8eb-126cd911135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Each `batch[i]` is a dictionary with a \"messages\" field\n",
    "    refs = [sample[\"messages\"][2] for sample in batch]\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            sample[\"messages\"][:2],  # assuming system + user\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for sample in batch\n",
    "    ]\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        padding_side='left',\n",
    "        truncation=True,\n",
    "    )\n",
    "    tokenized[\"ref\"] = refs\n",
    "    return tokenized\n",
    "\n",
    "# Step 2: Create DataLoader over a sliced dataset\n",
    "subset = dataset[\"test\"]\n",
    "loader = DataLoader(subset, batch_size=5, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3653a3d3-e419-4c5a-80d1-696868f16d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def parse_label_string(s): # parsing labels such as [1,2,3], [4]\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return [0]\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, int):\n",
    "            return [parsed]\n",
    "        elif isinstance(parsed, list):\n",
    "            return [int(x) for x in parsed]\n",
    "        else:\n",
    "            return [0]\n",
    "    except Exception:\n",
    "        return [0]\n",
    "\n",
    "def multi_label(response):\n",
    "    for line in response.split(\"\\n\"):\n",
    "        if \"reason\" in line:\n",
    "            matched = True\n",
    "            match = re.search(r'\"reason\":\\s*\"([^\"]*)\"', line)\n",
    "            if match:\n",
    "                reason_str = match.group(1)\n",
    "                return parse_label_string(reason_str)\n",
    "            else:\n",
    "                return [0]\n",
    "    if not matched:\n",
    "        return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb8e2f0-36be-43d9-a438-73f058804d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "total_cnt = 0\n",
    "correct_cnt = 0\n",
    "\n",
    "generated_responses = []\n",
    "references = []\n",
    "\n",
    "for batch in loader:\n",
    "    refs = batch.pop(\"ref\")\n",
    "    batch = {k: v.to(sft_model.device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = sft_model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Slice off prompt inputs to get only the generated completion\n",
    "    generated_ids = [\n",
    "        output[len(input_ids):]\n",
    "        for input_ids, output in zip(batch[\"input_ids\"], generated)\n",
    "    ]\n",
    "    \n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    generated_responses += responses\n",
    "    references += refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fa2ec84-b288-4245-8a54-b729ab40d302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resps = [multi_label(text) for text in generated_responses]\n",
    "refs = [multi_label(item['content']) for item in references]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83078c2a-099f-4018-bdf0-c7a0b5e1125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_true = mlb.fit_transform(refs)\n",
    "y_pred = mlb.transform(resps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c53ad958-c5ea-46a1-85c7-ee383ed76e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94       258\n",
      "           1       0.94      0.94      0.94       340\n",
      "           2       0.75      0.75      0.75        16\n",
      "           3       0.91      0.84      0.87        37\n",
      "           4       0.85      0.89      0.87        37\n",
      "           5       0.71      0.86      0.77        14\n",
      "           6       0.88      0.93      0.90        15\n",
      "           7       0.78      0.78      0.78         9\n",
      "           8       0.20      0.33      0.25         3\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       729\n",
      "   macro avg       0.77      0.81      0.79       729\n",
      "weighted avg       0.92      0.92      0.92       729\n",
      " samples avg       0.92      0.92      0.92       729\n",
      "\n",
      "Hamming Loss: 0.01962692619626926\n"
     ]
    }
   ],
   "source": [
    "class_labels = [str(label) for label in mlb.classes_]\n",
    "print(class_labels)\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=class_labels))\n",
    "hamming = hamming_loss(y_true, y_pred)\n",
    "print(f\"Hamming Loss: {hamming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bbb01-d0f5-4dfe-ad65-b0b5faad6bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12 (fu)",
   "language": "python",
   "name": "python312_torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
